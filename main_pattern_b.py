#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èª­å”‡è¡“ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Pattern Bå¯¾å¿œãƒ»è©•ä¾¡æŒ‡æ¨™çµ±ä¸€ç‰ˆ)
- Pattern B: CNN â†’ LSTM â†’ Temporal Attention
- Sigmoid/Softmaxç°¡å˜åˆ‡ã‚Šæ›¿ãˆ
- vowelãƒ¢ãƒ‡ãƒ«ã¯ Attention ãªã—(NoAttn)ç‰ˆã«å·®ã—æ›¿ãˆå¯èƒ½
"""

import os
import argparse
import torch
from pathlib import Path
import numpy as np

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
print("ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
from phoneme_analysis_unified import analyze_phonemes_unified
from matrics_undefined import UnifiedEvaluationMetrics
from phoneme_aware_per import run_evaluation
try:
    from enhanced_metrics import EnhancedEvaluationMetrics
    ENHANCED_METRICS_AVAILABLE = True
except ImportError:
    print("Warning: enhanced_metrics not found. Using standard metrics only.")
    ENHANCED_METRICS_AVAILABLE = False
from dataset import create_dataloaders
from train import LipReadingTrainer, evaluate_model
from utils_pattern_b import (
    Config, set_seed, setup_logging, check_data_paths,
    check_gpu_availability, create_directories, save_results,
    print_model_info, MetricsCalculator, build_loaders_from_config, sync_num_classes_with_encoder
)
from ctc_analyzer import (
    analyze_blank_rate,
    analyze_consecutive_blanks, 
    analyze_phoneme_duration,
    analyze_blank_between_phonemes,
    visualize_ctc_analysis,
    print_analysis_summary
)
# ===== ãƒ¢ãƒ¼ãƒ‰éä¾å­˜ã®çµ±ä¸€ã‚µãƒãƒªãƒ¼å‡ºåŠ› =====
def _compute_first_last_accuracy(pred_seqs, tgt_seqs):
    n = 0; first_ok = 0; last_ok = 0
    for p, t in zip(pred_seqs, tgt_seqs):
        if len(t) == 0:
            continue
        n += 1
        if len(p) > 0 and p[0] == t[0]:
            first_ok += 1
        if len(p) > 0 and p[-1] == t[-1]:
            last_ok += 1
    if n == 0:
        return 0.0, 0.0
    return 100.0 * first_ok / n, 100.0 * last_ok / n

def print_unified_summary(final_raw, encoder):
    """
    final_raw: evaluate_modelã®æˆ»ã‚Šå€¤ã®rawéƒ¨ï¼ˆpredictions/targetså¿…é ˆï¼‰
    è¡¨ç¤ºã‚’ 'å­éŸ³å´ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ' ã«çµ±ä¸€ã—ã¦å‡ºåŠ›ã—ã€mmã«ã‚‚çµ±ä¸€ã‚­ãƒ¼ã‚’è¿½åŠ ã—ã¦è¿”ã™
    """
    from matrics_undefined import UnifiedEvaluationMetrics
    evalr = UnifiedEvaluationMetrics()

    preds = final_raw.get('predictions', [])
    tgts  = final_raw.get('targets', [])

    # 1) PER
    per = final_raw.get('per_per', final_raw.get('PER', None))
    if per is None:
        per = evalr.sequence_per(preds, tgts)  # ï¼…ã‚’è¿”ã™å®Ÿè£…æƒ³å®š

    # 2) å®Œå…¨ä¸€è‡´ç‡ï¼ˆç³»åˆ—ï¼‰
    exact = final_raw.get('exact_match_consonant_exact_match_rate',
                          final_raw.get('exact_match_vowel_rate', None))
    if exact is None:
        # collapseæ¸ˆã¿å‰æã®preds/tgtsãªã‚‰ãã®ã¾ã¾ã€æœªcollapseãªã‚‰å†…éƒ¨ã§collapseã™ã‚‹å®Ÿè£…ã«ä¾å­˜
        exact = evalr.sequence_exact_match_rate(preds, tgts)  # ï¼…ã‚’è¿”ã™å®Ÿè£…æƒ³å®š

    # 3) æœ€åˆ/æœ€å¾Œãƒˆãƒ¼ã‚¯ãƒ³æ­£è§£ç‡ï¼ˆãƒ¢ãƒ¼ãƒ‰éä¾å­˜ï¼‰
    # æ–‡å­—åˆ—ãƒªã‚¹ãƒˆãŒå‰æã®ã¯ãšã ãŒã€ã‚‚ã—intãªã‚‰encoderã§å¤‰æ›
    if preds and preds[0] and isinstance(preds[0][0], int):
        preds_tok = [encoder.ids_to_symbols(x) for x in preds]
        tgts_tok  = [encoder.ids_to_symbols(x) for x in tgts]
    else:
        preds_tok, tgts_tok = preds, tgts

    first_acc, last_acc = _compute_first_last_accuracy(preds_tok, tgts_tok)

    # ---- è¡¨ç¤ºï¼ˆå­éŸ³å´ã®ä½“è£ã«çµ±ä¸€ï¼‰----
    print(f"PER (éŸ³ç´ èª¤ã‚Šç‡):     {per:.2f}%")
    print(f"å®Œå…¨ä¸€è‡´ç‡ï¼ˆç³»åˆ—ï¼‰:    {exact:.2f}%")
    print(f"æœ€åˆ/æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³:   {first_acc:.2f}% / {last_acc:.2f}%")

    # mmã«çµ±ä¸€ã‚­ãƒ¼ã‚‚è¶³ã—ã¦è¿”ã™ï¼ˆä¿å­˜jsonã‚‚æƒã†ï¼‰
    final_raw.setdefault('per_per', per)
    final_raw['exact_match_sequence_rate'] = exact
    final_raw['position_first_accuracy'] = first_acc
    final_raw['position_last_accuracy']  = last_acc
    return final_raw

# =========================================================
# å¼•æ•°ã¨è¨­å®šå‡¦ç†
# =========================================================
def parse_arguments():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(description='èª­å”‡è¡“ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»è©•ä¾¡ (Pattern B)')

    parser.add_argument('--mode', type=str, choices=['train', 'eval', 'test'],
                        default='train', help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--config', type=str, default='config.yaml',
                        help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')

    parser.add_argument('--train_csv', type=str, help='è¨“ç·´ç”¨CSVãƒ‘ã‚¹')
    parser.add_argument('--valid_csv', type=str, help='æ¤œè¨¼ç”¨CSVãƒ‘ã‚¹')
    parser.add_argument('--test_csv', type=str, help='ãƒ†ã‚¹ãƒˆç”¨CSVãƒ‘ã‚¹')
    parser.add_argument('--checkpoint', type=str, help='ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹')

    # Attentionè¨­å®šï¼ˆå­éŸ³ãƒ¢ãƒ‡ãƒ«ã‚„æ³¨æ„ã‚ã‚Šãƒ¢ãƒ‡ãƒ«ã§æœ‰åŠ¹ï¼‰
    parser.add_argument('--attention-type', type=str, choices=['sigmoid', 'softmax'],
                        help='Attention type: sigmoid or softmax')
    parser.add_argument('--temperature', type=float, help='Attention temperature (0.1-1.0)')

    parser.add_argument('--use_softper', action='store_true',
                    help='Use SoftPER loss term (CTC + lambda*SoftPER)')
    parser.add_argument('--lambda_softper', type=float, default=None,
                        help='Weight for SoftPER term')
    parser.add_argument('--softper_tau', type=float, default=None,
                        help='Softmin temperature for SoftPER DP')

    # è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument('--epochs', type=int, help='ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch_size', type=int, help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--lr', type=float, help='å­¦ç¿’ç‡')

    parser.add_argument('--analyze_ctc', action='store_true', 
                    help='Analyze CTC predictions (blank rate, durations, etc.)')
    parser.add_argument('--ctc_analysis_output', type=str, default='ctc_analysis',
                    help='Output directory for CTC analysis results')

    # ãã®ä»–
    parser.add_argument('--device', type=str, choices=['cpu', 'cuda', 'auto'],
                        default='auto', help='ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹')
    parser.add_argument('--seed', type=int, default=42, help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰')
    parser.add_argument('--debug', action='store_true', help='ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰')
    return parser.parse_args()


def setup_config(args):
    """è¨­å®šã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    config = Config(args.config if os.path.exists(args.config) else None)

    # å¼•æ•°ä¸Šæ›¸ã
    if args.train_csv: config['data']['train_csv'] = args.train_csv
    if args.valid_csv: config['data']['valid_csv'] = args.valid_csv
    if args.test_csv:  config['data']['test_csv']  = args.test_csv
    if args.epochs:    config['training']['epochs'] = args.epochs
    if args.batch_size:config['data']['batch_size']  = args.batch_size
    if args.lr:        config['training']['lr']      = args.lr

    if args.attention_type: config['model']['attention_type'] = args.attention_type
    if args.temperature:    config['model']['temperature']    = args.temperature

    if args.device == 'auto':
        config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        config['device'] = args.device

    # SoftPER
    if 'training' not in config.config:
        config.config['training'] = {}

    if args.use_softper:
        config.config['training']['use_softper'] = True
    if args.lambda_softper is not None:
        config.config['training']['lambda_softper'] = float(args.lambda_softper)
    if args.softper_tau is not None:
        config.config['training']['softper_tau'] = float(args.softper_tau)

    config['debug'] = args.debug
    return config


# =========================================================
# ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆ
# =========================================================
def create_model_from_config(config, num_classes_from_encoder=None):
    """
    mode=='vowel' ã®å ´åˆã¯ Attentionãªã—ã® CompactVowelLipReader_NoAttn ã‚’ä½¿ç”¨
    mode=='consonant' ã®å ´åˆã¯ model_typeã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«é¸æŠ
    """
    mode = config['model'].get('mode', 'consonant')
    num_classes = int(num_classes_from_encoder or config['model']['num_classes'])

    if mode == 'vowel':
        from model_compact_vowel import CompactVowelLipReader_NoAttn
        return CompactVowelLipReader_NoAttn(
            num_classes=num_classes,
            dropout=config['model'].get('dropout_rate', 0.2),
        )
    
    else:  # consonant
        model_type = config['model'].get('model_type', 'deep_cnn')
        
        # ===== Deep CNN Model =====
        if model_type == 'deep_cnn':
            from model_deep_cnn import create_deep_cnn_model
            return create_deep_cnn_model(
                num_classes=num_classes,
                dropout=config['model']['dropout_rate'],
                lstm_layers=config['model'].get('lstm_layers', 3),
                lstm_hidden=config['model'].get('lstm_hidden', 256),
            )
        
        # ===== Pattern B (æ—¢å­˜) =====
        elif model_type == 'pattern_b_frame_attention':
            from model_pattern_b import create_improved_pattern_b_model
            return create_improved_pattern_b_model(
                num_classes=num_classes,
                dropout_rate=config['model']['dropout_rate'],
                attention_type=config['model']['attention_type'],
                temperature=config['model']['temperature'],
                dual_attention=config['model'].get('dual_attention', False)
            )
        
        else:
            raise ValueError(f"Unknown model_type: {model_type}")
    '''
    # main_pattern_b.py ã® create_model_from_config å†…
    if mode == 'consonant':
        from model_consonant_transformer import create_consonant_transformer_model
        return create_consonant_transformer_model(
            num_classes=num_classes,
            dropout=config['model']['dropout_rate'],
            d_model=config['model'].get('d_model', 256),
            nhead=config['model'].get('nhead', 8),
            num_layers=config['model'].get('num_layers', 4)
        )
    '''
# =========================================================
# å­¦ç¿’ãƒ¡ã‚¤ãƒ³
# =========================================================
def train_model(config, args):
    """ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
    config.print_attention_config()

    print("\nãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆä¸­...")
    train_loader, valid_loader, phoneme_encoder, labels = build_loaders_from_config(config.config)
    sync_num_classes_with_encoder(config.config, phoneme_encoder)
    
    mode = config.config['model'].get('mode', 'consonant')
    phoneme_type = 'æ¯éŸ³' if mode == 'vowel' else 'å­éŸ³'
    
    # ... ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆè¡¨ç¤º (æ—¢å­˜ã‚³ãƒ¼ãƒ‰) ...

    print("\nãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
    model = create_model_from_config(
        config.config,
        num_classes_from_encoder=phoneme_encoder.num_classes()
    )

    # â˜… ãƒ‡ãƒãƒƒã‚°è¿½åŠ 
    print(f"\n[DEBUG Model Output]")
    print(f"  encoder.num_classes(): {phoneme_encoder.num_classes()}")
    print(f"  config num_classes: {config.config['model']['num_classes']}")
    dummy_input = torch.randn(1, 40, 1, 64, 64).to(config['device'])
    model.to(config['device'])
    dummy_out = model(dummy_input)
    print(f"  model output shape: {dummy_out.shape}")  # (1, T, C) ã®Cã‚’ç¢ºèª

    # ... ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º (æ—¢å­˜ã‚³ãƒ¼ãƒ‰) ...

    # ===== Trainerä½œæˆ =====
    use_length_aware = config.config['training'].get('use_length_aware_loss', True)
    use_gradual_unfreeze = config.config['training'].get('gradual_unfreezing', True)
    
    print(f"\n{'='*60}")
    print(f"Trainer Configuration")
    print(f"{'='*60}")
    print(f"Length-Aware Loss: {use_length_aware}")
    print(f"Gradual Unfreezing: {use_gradual_unfreeze}")
    print(f"{'='*60}\n")
    
    mode = config.config['model'].get('mode', 'consonant')

    tr_cfg = config['training']
    use_softper = tr_cfg.get('use_softper', False)
    lambda_softper = tr_cfg.get('lambda_softper', 0.05)
    softper_tau = tr_cfg.get('softper_tau', 0.2)
    separate_softper_loss = tr_cfg.get('separate_softper_loss', True)

    print("\n" + "="*70)
    print("SoftPER Wiring Check (main -> trainer)")
    print("="*70)
    print(f"config.training.use_softper   = {tr_cfg.get('use_softper', None)} -> {use_softper}")
    print(f"config.training.lambda_softper = {lambda_softper}")
    print(f"config.training.softper_tau    = {softper_tau}")
    print("="*70 + "\n")

    trainer = LipReadingTrainer(
    model=model,
    phoneme_encoder=phoneme_encoder,
    device=config['device'],
    save_dir=config['save']['checkpoint_dir'],
    result_dir=config['save']['result_dir'],
    early_stopping_metric=config['training'].get('early_stopping_metric', 'val_loss'),
    use_length_aware_loss=use_length_aware,
    gradual_unfreezing=use_gradual_unfreeze,
    mode=mode,                                    # â† è¿½åŠ 
    use_softper=use_softper,                      # â† è¿½åŠ 
    lambda_softper=lambda_softper,                # â† è¿½åŠ 
    softper_tau=softper_tau,                      # â† è¿½åŠ 
    separate_softper_loss=separate_softper_loss,  # â† è¿½åŠ 
)
    
    # ===== ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š =====
    if use_gradual_unfreeze and 'unfreeze_schedule' in config.config['training']:
        custom_schedule = config.config['training']['unfreeze_schedule']
        
        # dictå‹ã«å¤‰æ›ï¼ˆYAMLã‹ã‚‰ã®èª­ã¿è¾¼ã¿ã§æ–‡å­—åˆ—ã‚­ãƒ¼ã«ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ï¼‰
        if custom_schedule:
            schedule_dict = {}
            for k, v in custom_schedule.items():
                epoch_num = int(k) if isinstance(k, str) else k
                schedule_dict[epoch_num] = str(v)
            
            print(f"\nğŸ”§ Setting CUSTOM unfreeze schedule from config.yaml...")
            trainer.set_unfreeze_schedule(schedule_dict)
    
    # Optimizerè¨­å®š
    trainer.setup_optimizer(
        optimizer_type=config['training']['optimizer'],
        lr=config['training']['lr'],
        weight_decay=config['training']['weight_decay']
    )
    
    # Schedulerè¨­å®š
    scheduler_params = config['training'].get('scheduler_params', {})
    trainer.setup_scheduler(
        scheduler_type=config['training']['scheduler'],
        **scheduler_params
    )

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿
    start_epoch = 0
    if args.checkpoint and os.path.exists(args.checkpoint):
        print(f"\nãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {args.checkpoint}")
        start_epoch = trainer.load_checkpoint(args.checkpoint)

    # ===== å­¦ç¿’é–‹å§‹å‰ã®æœ€çµ‚ç¢ºèª =====
    print(f"\n{'='*70}")
    print(f"Training Start Confirmation")
    print(f"{'='*70}")
    print(f"Model: {config.config['model'].get('model_type', 'unknown')}")
    print(f"Gradual Unfreezing: {use_gradual_unfreeze}")
    if use_gradual_unfreeze and trainer.unfreeze_schedule:
        print(f"Active Schedule Epochs: {sorted(trainer.unfreeze_schedule.keys())}")
    print(f"Total Epochs: {config['training']['epochs']}")
    print(f"Early Stopping Metric: {config['training'].get('early_stopping_metric', 'val_loss')}")
    print(f"{'='*70}\n")

    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    print("\n" + "=" * 70)
    print("è¨“ç·´é–‹å§‹")
    print("=" * 70)
    history = trainer.train(
        train_loader=train_loader,
        val_loader=valid_loader,
        epochs=config['training']['epochs'],
        early_stopping_patience=config['training'].get('early_stopping_patience', 20)
    )
    
    # ========== å­¦ç¿’æ›²ç·šä¿å­˜ ==========
    history_plot_path = os.path.join(config['save']['result_dir'], 'training_history.png')
    trainer.plot_history(save_path=history_plot_path)
    
    # ===============================================
    # âœ… çµ±ä¸€ã•ã‚ŒãŸæœ€çµ‚è©•ä¾¡ï¼ˆé€”ä¸­è©•ä¾¡ã¨åŒä¸€Evaluatorã‚’ä½¿ç”¨ï¼‰
    # ===============================================
    print("\n" + "=" * 70)
    print("æœ€çµ‚è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ä¸­ï¼ˆvalidateã¨åŒä¸€ãƒ­ã‚¸ãƒƒã‚¯ï¼‰...")
    print("=" * 70)
    
    final = evaluate_model(
        model, valid_loader, phoneme_encoder, config['device'],
        show_samples=True, num_samples=10
    )
    mm = print_unified_summary(final['raw'], phoneme_encoder)
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    evaluator = UnifiedEvaluationMetrics()
    print("\nã‚µãƒ³ãƒ—ãƒ«çµæœï¼ˆæ¯éŸ³/å­éŸ³å…±é€šï¼‰:")
    evaluator.print_sample_results(
        final['raw']['predictions'],
        final['raw']['targets'],
        num_samples=10,
        apply_collapse=True,
        show_correct=True,
        show_incorrect=True,
        vowel_mode=(mode == 'vowel')
    )
    
    # éŸ³ç´ åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    evaluator.print_per_phoneme_metrics(
        final['raw']['predictions'],
        final['raw']['targets'],
        labels,
        mode=mode
    )
    
    # æœ€çµ‚è©•ä¾¡ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print(" æœ€çµ‚è©•ä¾¡ã‚µãƒãƒªãƒ¼")
    print("=" * 70)
    
    per_val = mm.get('per_per', mm.get('PER', 0.0))
    print(f"PER (éŸ³ç´ èª¤ã‚Šç‡):     {per_val:.2f}%")
    
    if mode == 'consonant':
        print(f"å­éŸ³å®Œå…¨ä¸€è‡´ç‡:       {mm.get('exact_match_consonant_exact_match_rate', 0.0):.2f}%")
        print(f"æœ€åˆ/æœ€å¾Œã®å­éŸ³æ­£è§£ç‡: {mm.get('position_first_accuracy', 0.0):.2f}% / {mm.get('position_last_accuracy', 0.0):.2f}%")
    else:
        if 'exact_match_vowel_rate' in mm:
            print(f"æ¯éŸ³å®Œå…¨ä¸€è‡´ç‡:       {mm['exact_match_vowel_rate']:.2f}%")
        if 'position_first_accuracy' in mm:
            print(f"æœ€åˆ/æœ€å¾Œã®æ¯éŸ³æ­£è§£ç‡: {mm.get('position_first_accuracy', 0.0):.2f}% / {mm.get('position_last_accuracy', 0.0):.2f}%")

    # â˜… éŸ³éŸ¿çš„PERè©•ä¾¡
    if ENHANCED_METRICS_AVAILABLE:
        print("\n" + "=" * 70)
        print(" éŸ³éŸ¿çš„é¡ä¼¼åº¦ã‚’è€ƒæ…®ã—ãŸè©•ä¾¡")
        print("=" * 70)
        
        evaluator_acoustic = EnhancedEvaluationMetrics(
            use_acoustic=True,
            mode=mode,
            phoneme_encoder=phoneme_encoder
        )
        
        acoustic_result = evaluator_acoustic.calculate_acoustic_per(
            predictions=final['raw']['predictions'],
            targets=final['raw']['targets'],
            apply_collapse=True
        )
        
        print(f"æ¨™æº–PER:           {acoustic_result['standard_per']:.2f}%")
        print(f"éŸ³éŸ¿çš„PER:         {acoustic_result['acoustic_per']:.2f}%")
        diff = acoustic_result['standard_per'] - acoustic_result['acoustic_per']
        print(f"å·®åˆ†:              {diff:+.2f}% {'(é¡ä¼¼éŸ³ç´ ã®æ··åŒãŒå¤šã„)' if diff > 5 else ''}")
        print(f"\nã‚¨ãƒ©ãƒ¼å†…è¨³:")
        print(f"  ç½®æ›:     {acoustic_result['substitutions']:,} (é‡ã¿ä»˜ã: {acoustic_result['weighted_substitutions']:.2f})")
        print(f"  å‰Šé™¤:     {acoustic_result['deletions']:,}")
        print(f"  æŒ¿å…¥:     {acoustic_result['insertions']:,}")
        print(f"  ç·éŸ³ç´ æ•°: {acoustic_result['total_phonemes']:,}")

    # ========== è©•ä¾¡çµæœçµ±åˆä¿å­˜ ==========
    from utils_pattern_b import save_evaluation_report
    save_evaluation_report(
        metrics_dict=mm,
        predictions=final['raw']['predictions'],
        targets=final['raw']['targets'],
        labels=labels,
        save_dir=config['save']['result_dir'],
        mode=mode,
        sample_results=final['raw'].get('sample_results', [])
    )
    
    # ========== éŸ³ç´ ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸPERè¨ˆç®—ï¼ˆè¿½åŠ ï¼‰ ==========
    print("\n" + "=" * 70)
    print("éŸ³ç´ ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸPERè¨ˆç®—ä¸­...")
    print("=" * 70)
    
    try:
        from phoneme_aware_per import run_evaluation
        
        per_results = run_evaluation(
            predictions_list=final['raw']['predictions'],
            targets_list=final['raw']['targets'],
            result_dir=config['save']['result_dir']
        )
        
        # ===== ä¿®æ­£ï¼šã™ã§ã«%ãªã®ã§100å€ã—ãªã„ =====
        print(f"âœ“ Phoneme-Aware PER: {per_results['overall']['phoneme_aware_per']:.2f}%")
        print(f"âœ“ Hard PER:          {per_results['overall']['hard_per']:.2f}%")
        print(f"âœ“ æ”¹å–„ç‡:            {per_results['overall']['improvement_rate']:.2f}%")
        
    except Exception as e:
        print(f"âš  éŸ³ç´ ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸPERè¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    # ===============================================
    # Attentionå¯è¦–åŒ–
    # ===============================================
    try:
        print("\n" + "=" * 70)
        print("Attentionå¯è¦–åŒ– + ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡")
        print("=" * 70)
        from attention_visualizer import visualize_attention_with_samples

        has_attn_attr = hasattr(model, "attention_weights")
        if mode == 'vowel' and not has_attn_attr:
            print("ï¼ˆNoAttnãƒ¢ãƒ‡ãƒ«ã®ãŸã‚Attentionå¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        else:
            attention_result = visualize_attention_with_samples(
                model=model,
                data_loader=valid_loader,
                phoneme_encoder=phoneme_encoder,
                device=config['device'],
                num_samples=5,
                save_dir=config['save']['result_dir']
            )
            print(f"\nâœ“ Attentionå¯è¦–åŒ–å®Œäº†")
            print(f"  - å¯è¦–åŒ–ç”»åƒ: {len(attention_result['correct_samples']) + len(attention_result['incorrect_samples'])}æš")
            print(f"  - æ­£è§£ç‡: {attention_result['accuracy']*100:.1f}%")
    except Exception as e:
        print(f"âš  Attentionå¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback; traceback.print_exc()

    # ========== éŸ³ç´ åˆ¥è©³ç´°åˆ†æï¼ˆãƒ«ãƒ¼ãƒˆç›´ä¸‹ã«ä¿å­˜ï¼‰ ==========
    print("\néŸ³ç´ åˆ¥è©³ç´°åˆ†æã‚’å®Ÿè¡Œä¸­...")
    analyze_phonemes_unified(
        predictions=final['raw'].get('predictions', []),
        targets=final['raw'].get('targets', []),
        labels=labels,
        save_dir=config['save']['result_dir'],  # ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ãªã—
        apply_collapse=True
    )

    # ========== CTCåˆ†æ ==========
    if config.config.get('analysis', {}).get('analyze_ctc', False):
        print("\n" + "="*60)
        print("Starting CTC Analysis...")
        print("="*60)
        
        # ãƒãƒƒãƒæ•°ã‚’è¨­å®šã‹ã‚‰å–å¾—
        num_batches = config.config.get('analysis', {}).get('ctc_num_batches', 10)
        
        model.eval()
        all_outputs = []
        all_decoded = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(valid_loader):
                # ãƒãƒƒãƒãŒè¾æ›¸å½¢å¼ã®å ´åˆ
                if isinstance(batch, dict):
                    videos = batch['video']
                    input_lengths = batch['input_length']
                else:
                    videos = batch[0]
                    input_lengths = batch[2]
                
                videos = videos.to(config['device'])
                outputs = model(videos)  # (B, T, C)
                
                # log_probsã«å¤‰æ›
                log_probs = torch.log_softmax(outputs, dim=-1)  # (B, T, C)
                
                # â˜… (B, T, C) â†’ (T, B, C) ã«å¿…ãšå¤‰æ›
                log_probs = log_probs.permute(1, 0, 2)  # (T, N, C)
                
                all_outputs.append(log_probs.cpu())
                
                # ctc_greedy_decodeã‚’ä½¿ç”¨
                
                from utils_pattern_b import ctc_greedy_decode, ids_to_phonemes
                
                decoded_ids = ctc_greedy_decode(
                    log_probs,
                    blank_id=phoneme_encoder.blank_id,
                    input_lengths=input_lengths
                )

                # ctc_greedy_decodeã‚’ä½¿ç”¨
                from utils_pattern_b import ctc_greedy_decode
                
                decoded_ids = ctc_greedy_decode(
                    log_probs,
                    blank_id=phoneme_encoder.blank_id,
                    input_lengths=input_lengths
                )
                
                all_decoded.extend(decoded_ids)  # IDåˆ—ã‚’ä¿å­˜
                
                if batch_idx >= num_batches - 1:
                    break
        
        
        # å…¨ãƒãƒƒãƒã‚’çµåˆ
        all_outputs = torch.cat(all_outputs, dim=1)  # (T, N_total, C)
        
        # åˆ†æå®Ÿè¡Œ
        results = {
            'blank_rate': analyze_blank_rate(all_outputs, blank_id=0),
            'consecutive_blanks': analyze_consecutive_blanks(all_outputs, blank_id=0),
            'phoneme_duration': analyze_phoneme_duration(all_outputs, all_decoded, blank_id=0),
            'blanks_between': analyze_blank_between_phonemes(all_outputs, blank_id=0)
        }
        
        # çµæœå‡ºåŠ›
        print_analysis_summary(results)
        
        # å¯è¦–åŒ–ä¿å­˜
        ctc_output_dir = os.path.join(config['save']['result_dir'], 'ctc_analysis')
        os.makedirs(ctc_output_dir, exist_ok=True)
        visualize_ctc_analysis(results, 
                            os.path.join(ctc_output_dir, 'ctc_analysis.png'))
        
        print(f"\nAnalysis plot saved to: {ctc_output_dir}/ctc_analysis.png")

    print("\n" + "=" * 70)
    print("âœ“ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
    print("=" * 70)


# =========================================================
# è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰
# =========================================================
def evaluate_model_mode(config, args):
    """è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰"""
    print("\n" + "=" * 70)
    print("è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰")
    print("=" * 70)

    # DataLoaderä½œæˆ
    print("\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆä¸­...")
    mode = config['model'].get('mode', 'consonant')
    _, test_loader, phoneme_encoder, labels = create_dataloaders(
        train_csv_path=config['data'].get('train_csv'),
        valid_csv_path=config['data']['test_csv'],
        batch_size=config['data']['batch_size'],
        num_workers=config['data']['num_workers'],
        augmentation_config=None,
        max_length=config['data'].get('max_length', 40),
        mode=mode,
    )
    
    sync_num_classes_with_encoder(config.config, phoneme_encoder)
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»èª­ã¿è¾¼ã¿
    print(f"\nãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­... (mode={mode}, num_classes={phoneme_encoder.num_classes()})")
    model = create_model_from_config(
        config.config,
        num_classes_from_encoder=phoneme_encoder.num_classes()
    )
    model.to(config['device'])
    
    if not args.checkpoint:
        raise ValueError("è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ(--checkpoint)ãŒå¿…é ˆã§ã™")
    if not os.path.exists(args.checkpoint):
        raise FileNotFoundError(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.checkpoint}")

    print(f"\nãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=config['device'])
    
    # å‡ºåŠ›æ¬¡å…ƒãƒã‚§ãƒƒã‚¯
    if 'model_state_dict' in checkpoint:
        head_keys = [k for k in checkpoint['model_state_dict'].keys() if k.endswith('classifier.3.weight')]
        if head_keys:
            ckpt_out = checkpoint['model_state_dict'][head_keys[0]].shape[0]
            if ckpt_out != phoneme_encoder.num_classes():
                raise ValueError(
                    f"Checkpointå‡ºåŠ›æ¬¡å…ƒ({ckpt_out})ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€({phoneme_encoder.num_classes()})ãŒä¸ä¸€è‡´ã§ã™ã€‚"
                )
        model.load_state_dict(checkpoint['model_state_dict'], strict=True)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    model.eval()
    
    # è©•ä¾¡å®Ÿè¡Œ
    print("\næœ€çµ‚è©•ä¾¡ã‚’å®Ÿè¡Œä¸­...")
    final = evaluate_model(
        model, test_loader, phoneme_encoder, config['device'],
        show_samples=True, num_samples=10
    )
    mm = print_unified_summary(final['raw'], phoneme_encoder)
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    phoneme_type = 'æ¯éŸ³' if mode == 'vowel' else 'å­éŸ³'
    print(f"\nã‚µãƒ³ãƒ—ãƒ«çµæœï¼ˆ{phoneme_type}ãƒ¢ãƒ¼ãƒ‰ï¼‰:")
    evaluator = UnifiedEvaluationMetrics()
    evaluator.print_sample_results(
        final['raw']['predictions'],
        final['raw']['targets'],
        num_samples=10,
        apply_collapse=True,
        show_correct=True,
        show_incorrect=True,
        vowel_mode=(mode == 'vowel')
    )
    
    # â˜… éŸ³ç´ åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    # éŸ³ç´ åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    evaluator = UnifiedEvaluationMetrics()
    evaluator.print_per_phoneme_metrics(
        final['raw']['predictions'],
        final['raw']['targets'],
        labels,
        mode=mode
    )
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡çµæœ")
    print("=" * 70)
    per_val = mm.get('per_per', mm.get('PER', 0.0))
    print(f"PER (éŸ³ç´ èª¤ã‚Šç‡):     {per_val:.2f}%")

    if mode == 'consonant':
        print(f"å®Œå…¨ä¸€è‡´ç‡ï¼ˆå­éŸ³åˆ—ï¼‰:  {mm.get('exact_match_consonant_exact_match_rate', 0.0):.2f}%")
    else:
        if 'exact_match_vowel_rate' in mm:
            print(f"å®Œå…¨ä¸€è‡´ç‡ï¼ˆæ¯éŸ³åˆ—ï¼‰:  {mm.get('exact_match_vowel_rate', 0.0):.2f}%")
    
    # éŸ³ç´ åˆ¥åˆ†æ
    analysis_dir = os.path.join(config['save']['result_dir'], 'phoneme_analysis')
    os.makedirs(analysis_dir, exist_ok=True)

    analysis = analyze_phonemes_unified(
        predictions=final['raw'].get('predictions', []),
        targets=final['raw'].get('targets', []),
        labels=labels,
        save_dir=analysis_dir,
        top_k=5,
        plot_confusion=True,
    )

    print("\n--- éŸ³ç´ åˆ¥åˆ†æ ---")
    print(f"Overall Acc: {analysis.get('overall_accuracy', 0.0)*100:.2f}%")
    print(f"Macro   Acc: {analysis.get('macro_accuracy', 0.0)*100:.2f}%")

    # â˜… éŸ³éŸ¿çš„PERè©•ä¾¡
    if ENHANCED_METRICS_AVAILABLE:
        print("\n" + "=" * 70)
        print(" éŸ³éŸ¿çš„é¡ä¼¼åº¦ã‚’è€ƒæ…®ã—ãŸè©•ä¾¡")
        print("=" * 70)
        
        evaluator_acoustic = EnhancedEvaluationMetrics(
            use_acoustic=True,
            mode=mode,
            phoneme_encoder=phoneme_encoder
        )
        
        acoustic_result = evaluator_acoustic.calculate_acoustic_per(
            predictions=final['raw']['predictions'],
            targets=final['raw']['targets'],
            apply_collapse=True
        )
        
        print(f"æ¨™æº–PER:           {acoustic_result['standard_per']:.2f}%")
        print(f"éŸ³éŸ¿çš„PER:         {acoustic_result['acoustic_per']:.2f}%")
        diff = acoustic_result['standard_per'] - acoustic_result['acoustic_per']
        print(f"å·®åˆ†:              {diff:+.2f}% {'(é¡ä¼¼éŸ³ç´ ã®æ··åŒãŒå¤šã„)' if diff > 5 else ''}")
        print(f"\nã‚¨ãƒ©ãƒ¼å†…è¨³:")
        print(f"  ç½®æ›:     {acoustic_result['substitutions']:,} (é‡ã¿ä»˜ã: {acoustic_result['weighted_substitutions']:.2f})")
        print(f"  å‰Šé™¤:     {acoustic_result['deletions']:,}")
        print(f"  æŒ¿å…¥:     {acoustic_result['insertions']:,}")
        print(f"  ç·éŸ³ç´ æ•°: {acoustic_result['total_phonemes']:,}")

        # ========== éŸ³ç´ ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸPERè¨ˆç®—ï¼ˆè¿½åŠ ï¼‰ ==========
    print("\n" + "=" * 70)
    print("éŸ³ç´ ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸPERè¨ˆç®—ä¸­...")
    print("=" * 70)
    
    try:
        from phoneme_aware_per import run_evaluation
        
        per_results = run_evaluation(
            predictions_list=final['raw']['predictions'],
            targets_list=final['raw']['targets'],
            result_dir=config['save']['result_dir']
        )
    except Exception as e:
        print(f"âš  ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ“ è©•ä¾¡å®Œäº†")
    return mm

# =========================================================
# ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
# =========================================================
def main():
    args = parse_arguments()
    config = setup_config(args)
    set_seed(config['seed'])
    check_gpu_availability()
    create_directories(config)

    if not check_data_paths(config):
        print("ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    if args.mode == 'train':
        train_model(config, args)
    elif args.mode in ['eval', 'test']:
        evaluate_model_mode(config, args)
    else:
        raise ValueError(f"æœªçŸ¥ã®ãƒ¢ãƒ¼ãƒ‰: {args.mode}")

    print("\nå‡¦ç†å®Œäº†")

    

if __name__ == "__main__":
    main()